# ğŸš€ USS-Stack â€“ Lokales RAG-System mit Pi & Ollama

**â€Ich baue Systeme, die sich selbst sichern, schÃ¼tzen und weiterentwickeln.â€œ**

USS-Stack (Unmanned Self-sustaining Ship) ist ein modular aufgebautes, agentenbasiertes RAG-Framework, das vollstÃ¤ndig lokal betrieben wird â€“ auf Raspberry Pi-GerÃ¤ten und einem GPU-Windows-Tower mit Ollama.  
Ziel ist ein selbstheilendes, bootfestes System mit intelligenter Automatisierung fÃ¼r Updates, RAID-Ãœberwachung, Backups und vektorbasierte KI-Abfragen.

---

## ğŸ§  Kernfeatures auf einen Blick

- ğŸ“¦ **Unattended Upgrades** mit `ntfy` Push-Benachrichtigungen  
- ğŸ§² **RAID-Monitoring inkl. SMART-Checks & Emoji-Fazit**  
- ğŸ” **Backup-Agent mit rclone, Tar A/B-Prinzip & sha256-PrÃ¼fsummen**  
- ğŸ§¬ **Qdrant VectorDB mit Ã¼ber 100k Code-Chunks**  
- ğŸ¤– **FastAPI-RAG-Endpoint (`/ask`) zur LLM-Abfrage via Ollama**  
- ğŸ§  **Lokaler Ollama-LLM als Chat-Backend auf GPU-Server**  
- ğŸ” **Bootfeste Einrichtung via systemd & NSSM (Windows Services)**

---

## ğŸ—ï¸ ArchitekturÃ¼bersicht

```text
[agent-pi] --(ntfy/systemd)--> [FastAPI/RAG] --> [Ollama (GPU-Tower)]
    |                             â–²
    |                             |
[raid-pi] <--- RAID / Backup ----+
       \
        > Qdrant (Vector Search)


âš™ï¸ Technologien & Komponenten
Raspberry Pi (Agent-Node & RAID-Node)

Qdrant fÃ¼r Vektorsuche

FastAPI, uvicorn, Python 3, venv

rclone, smartmontools, ntfy

Ollama (lokaler LLM-Server auf Windows per nssm)

systemd-Timer & Dienste

ğŸ› ï¸ Setup & Installation
Detaillierte Installationsschritte findest du im unteren Teil des Repos (README oder /docs/-Ordner).

FÃ¼r Schnellstarter:

# Pi: Basis-Tools & Agent-Skripte
sudo apt install unattended-upgrades ntfy rclone smartmontools

# Qdrant starten (Docker)
cd /opt/qdrant
docker compose up -d

# FastAPI-RAG-Service aktivieren
cd /opt/uss_api
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt
systemctl enable uss-api.service uss-ready.service

# Windows: Ollama-Service (LLM)
winget install nomic.gpt4all
nssm install Ollama "...\ollama.exe" serve
ğŸ” Beispiel: RAG-Abfrage

curl -X POST http://<pi-ip>:8000/ask \
     -H "Content-Type: application/json" \
     -d '{"query":"Wie registriere ich ein Modul?","top_k":4}'
ğŸ“¦ Antwort: JSON mit answer + sources direkt aus deinem Code-Repo.

ğŸ“œ Lizenz
Dieses Projekt steht unter CC BY-NC-SA 4.0.
â†’ Siehe LICENSE fÃ¼r Details.

ğŸ›£ï¸ Roadmap (Next Steps)
 MMR + Cross-Encoder Re-Ranking

 Deutsch-getuntes Modell (mistral-7b-de)

 Automatisches Cleanup alter Backups

 GitHub-Monorepo mit klarer Trennung: api/, infra/, embeddings/, docs/

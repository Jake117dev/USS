# ğŸš€ USS-Stack â€“ Lokales RAG-System mit Pi & Ollama

**â€Ich baue Systeme, die sich selbst sichern, schÃ¼tzen und weiterentwickeln.â€œ**

USS-Stack (Unmanned Self-sustaining Ship) ist ein modular aufgebautes, agentenbasiertes RAG-Framework, das vollstÃ¤ndig lokal betrieben wird â€“ auf Raspberry Pi-GerÃ¤ten und einem GPU-Windows-Tower mit Ollama.  
Ziel ist ein selbstheilendes, bootfestes System mit intelligenter Automatisierung fÃ¼r Updates, RAID-Ãœberwachung, Backups und vektorbasierte KI-Abfragen.

---

## ğŸ§  Kernfeatures auf einen Blick

- ğŸ“¦ **Unattended Upgrades** mit `ntfy` Push-Benachrichtigungen  
- ğŸ§² **RAID-Monitoring inkl. SMART-Checks & Emoji-Fazit**  
- ğŸ” **Backup-Agent mit rclone, Tar A/B-Prinzip & sha256-PrÃ¼fsummen**  
- ğŸ§¬ **Qdrant VectorDB mit Ã¼ber 100k Code-Chunks**  
- ğŸ¤– **FastAPI-RAG-Endpoint (`/ask`) zur LLM-Abfrage via Ollama**  
- ğŸ§  **Lokaler Ollama-LLM als Chat-Backend auf GPU-Server**  
- ğŸ” **Bootfeste Einrichtung via systemd & NSSM (Windows Services)**

---

## ğŸ—ï¸ ArchitekturÃ¼bersicht

```
[agent-pi] --(ntfy/systemd)--> [FastAPI/RAG] --> [Ollama (GPU-Tower)]
    |                             â–²
    |                             |
[raid-pi] <--- RAID / Backup ----+
       \
        > Qdrant (Vector Search)
```

---

## âš™ï¸ Technologien & Komponenten

- `Raspberry Pi` (Agent-Node & RAID-Node)
- `Qdrant` fÃ¼r Vektorsuche
- `FastAPI`, `uvicorn`, `Python 3`, `venv`
- `rclone`, `smartmontools`, `ntfy`
- `Ollama` (lokaler LLM-Server auf Windows per `nssm`)
- `systemd`-Timer & Dienste

---

## ğŸ› ï¸ Setup & Installation

### ğŸ“¦ Raspberry Pi (agent-pi & raid-pi)

```bash
sudo apt update && sudo apt install unattended-upgrades ntfy rclone smartmontools
# 50unattended-upgrades anpassen, ntfy-Hook in /etc/apt/apt.conf.d/
# raid_report.sh + backup_agent.sh nach /usr/local/bin/, cron/timer einrichten
```

### ğŸ§¬ Qdrant (VectorDB)

```bash
cd /opt/qdrant
docker compose up -d
```

### ğŸ§  FastAPI RAG-Endpoint

```bash
cd /opt/uss_api
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
systemctl enable uss-api.service uss-ready.service
systemctl start uss-api.service
```

### ğŸªŸ Windows (Ollama LLM-Server)

```powershell
winget install nomic.gpt4all
setx OLLAMA_HOST 0.0.0.0
setx OLLAMA_MODELS "%USERPROFILE%\.ollama\models"
nssm install Ollama "C:\Users\mrieh\AppData\Local\Programs\Ollama\ollama.exe" serve
nssm set Ollama AppDirectory "C:\Users\mrieh\AppData\Local\Programs\Ollama"
nssm start Ollama
```

---

## ğŸ” Nutzung

### Beispiel-Query an den `/ask`-Endpoint:

```bash
curl -X POST http://<pi-ip>:8000/ask \
     -H "Content-Type: application/json" \
     -d '{"query":"Wie registriere ich ein Modul?","top_k":4}'
```

â¡ï¸ Antwort: JSON mit `answer` + `sources` direkt aus deinem Code-Repo.

---

## ğŸ“œ Lizenz

Dieses Projekt steht unter der [Creative Commons BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).  
Siehe [LICENSE](./LICENSE) fÃ¼r Details.

---

## ğŸ›£ï¸ Roadmap (Next Steps)

- [ ] MMR + Cross-Encoder Re-Ranking  
- [ ] Deutsch-getuntes Modell (`mistral-7b-de`)  
- [ ] Automatisches Cleanup alter Backups  
- [ ] GitHub-Monorepo mit klarer Struktur: `api/`, `infra/`, `embeddings/`, `docs/`

---

ğŸ’¬ **Fragen oder Ideen?** â†’ Ã–ffne ein Issue oder schick 'nen Pull Request.  
ğŸ§­ Bleib neugierig â€“ das USS fÃ¤hrt von selbst weiter ğŸ˜‰
